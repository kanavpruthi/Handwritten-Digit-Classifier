{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kanav\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Kanav\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Kanav\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Kanav\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Kanav\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Kanav\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Kanav\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Kanav\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Kanav\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Kanav\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Kanav\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Kanav\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(888)\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(404)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from time import strftime\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN_PATH='MNIST/digit_xtrain.csv'\n",
    "X_TEST_PATH='MNIST/digit_xtest.csv'\n",
    "Y_TRAIN_PATH='MNIST/digit_ytrain.csv'\n",
    "Y_TEST_PATH='MNIST/digit_ytest.csv'\n",
    "NUM_CLASSES=10\n",
    "VALIDATION_SIZE=10000\n",
    "\n",
    "LOGGING_PATH='tensor_mnist_digit_logs/'\n",
    "\n",
    "IMAGE_HEIGHT=28\n",
    "IMAGE_WIDTH=28\n",
    "CHANNELS=1\n",
    "TOTAL_INPUTS=IMAGE_HEIGHT*IMAGE_WIDTH*CHANNELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_all=np.loadtxt(Y_TRAIN_PATH, dtype='int', delimiter=',')\n",
    "y_test=np.loadtxt(Y_TEST_PATH, dtype='int', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 30.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "x_train_all=np.loadtxt(X_TRAIN_PATH, dtype=int, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_test=np.loadtxt(X_TEST_PATH, dtype=int, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The np.eye(n) is a diagonal matrix of diag elements = 1\n",
    "\n",
    "x_test, x_train_all = x_test/255.0, x_train_all/255.0\n",
    "\n",
    "y_test=np.eye(NUM_CLASSES)[y_test]\n",
    "y_train_all=np.eye(NUM_CLASSES)[y_train_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Validation DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val=x_train_all[:VALIDATION_SIZE]\n",
    "x_train=x_train_all[VALIDATION_SIZE:]\n",
    "\n",
    "\n",
    "y_val=y_train_all[:VALIDATION_SIZE]\n",
    "y_train=y_train_all[VALIDATION_SIZE:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up TensorFlow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=tf.placeholder(dtype=tf.float32, shape=[None, TOTAL_INPUTS], name='X')\n",
    "Y=tf.placeholder(dtype=tf.float32, shape=[None, NUM_CLASSES],  name='labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Architecture\n",
    "\n",
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "learning_rate=0.001\n",
    "\n",
    "n_hidden1=512\n",
    "n_hidden2=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_layer(inp, weight_dim, bias_dim, name):\n",
    "    with tf.name_scope(name):\n",
    "        initial_w=tf.truncated_normal(shape=weight_dim, seed=42, stddev=0.1)\n",
    "        w=tf.Variable(initial_w, name='W')\n",
    "        \n",
    "        initial_b=tf.constant(shape=bias_dim, value=0.0)\n",
    "        b=tf.Variable(initial_b, name='W')\n",
    "\n",
    "        layer_in=tf.matmul(inp, w) +b\n",
    "        \n",
    "        if name=='out':\n",
    "            output=tf.nn.softmax(layer_in)\n",
    "        else:\n",
    "            output=tf.nn.relu(layer_in)\n",
    "            \n",
    "        tf.summary.histogram('weights', w)\n",
    "        tf.summary.histogram('biases', b)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_1=setup_layer(X, weight_dim=[TOTAL_INPUTS, n_hidden1], bias_dim=[n_hidden1], name='Layer_1')\n",
    "# layer_2=setup_layer(layer_1, weight_dim=[n_hidden1, n_hidden2], bias_dim=[n_hidden2], name='Layer_2')\n",
    "# output=setup_layer(layer_2, weight_dim=[n_hidden2, NUM_CLASSES], bias_dim=[NUM_CLASSES], name='out')\n",
    "\n",
    "# model_name=f'{n_hidden1} - {n_hidden2} LR{learning_rate} E{epochs}' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1=setup_layer(X, weight_dim=[TOTAL_INPUTS, n_hidden1], bias_dim=[n_hidden1], name='Layer_1')\n",
    "layer_drop=tf.nn.dropout(layer_1, keep_prob=0.8)\n",
    "layer_2=setup_layer(layer_drop, weight_dim=[n_hidden1, n_hidden2], bias_dim=[n_hidden2], name='Layer_2')\n",
    "output=setup_layer(layer_2, weight_dim=[n_hidden2, NUM_CLASSES], bias_dim=[NUM_CLASSES], name='out')\n",
    "\n",
    "model_name=f'{n_hidden1} - {n_hidden2} LR{learning_rate} E{epochs}' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created directory\n"
     ]
    }
   ],
   "source": [
    "folder_name = f'{model_name} at {strftime(\"%H %M\")}'\n",
    "directory=os.path.join(LOGGING_PATH, folder_name)\n",
    "try:\n",
    "    os.makedirs(directory)\n",
    "except OSError as e:\n",
    "    print(str(e))\n",
    "else:\n",
    "    print('Successfully created directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss, Optimisation, and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cross entropy loss function shall be used\n",
    "with tf.name_scope('loss_calc'):\n",
    "    loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=output , labels=Y)) ##defining loss fn\n",
    "## defining the optimiser we gonna use\n",
    "with tf.name_scope('optimizer'):\n",
    "    optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_step=optimizer.minimize(loss)## defining the parameter that optimizer minimises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy_calc'):\n",
    "    correct_pred = tf.equal(tf.argmax(output, axis=1), tf.argmax(Y, axis=1))\n",
    "    accuracy= tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('performance'):\n",
    "\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('cost', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Input Images in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Show_Image'):\n",
    "\n",
    "    x_image=tf.reshape(X, [-1,28,28,1])\n",
    "\n",
    "    tf.summary.image('image_input', x_image, max_outputs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Running Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess=tf.Session()\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup File Writer and Merging Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_summary=tf.summary.merge_all()\n",
    "\n",
    "train_writer=tf.summary.FileWriter(directory+'/train')\n",
    "train_writer.add_graph(sess.graph)\n",
    "\n",
    "validation_writer=tf.summary.FileWriter(directory+'/validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_of_batch=1000\n",
    "\n",
    "num_examples=y_train.shape[0]\n",
    "num_iterations = int(num_examples/size_of_batch)\n",
    "\n",
    "index_in_epoch=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(batch_size, data, labels):\n",
    "    \n",
    "    global num_examples\n",
    "    global index_in_epoch\n",
    "    \n",
    "    start=index_in_epoch\n",
    "    index_in_epoch+=batch_size\n",
    "    \n",
    "    if index_in_epoch>num_examples:\n",
    "        start=0\n",
    "        index_in_epoch=batch_size\n",
    "    \n",
    "    end=index_in_epoch\n",
    "    \n",
    "    return data[start: end], labels[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0   | Training Accuracy: 0.8500000238418579\n",
      "Epoch 1   | Training Accuracy: 0.8600000143051147\n",
      "Epoch 2   | Training Accuracy: 0.8679999709129333\n",
      "Epoch 3   | Training Accuracy: 0.8700000047683716\n",
      "Epoch 4   | Training Accuracy: 0.8730000257492065\n",
      "Epoch 5   | Training Accuracy: 0.9700000286102295\n",
      "Epoch 6   | Training Accuracy: 0.9739999771118164\n",
      "Epoch 7   | Training Accuracy: 0.9819999933242798\n",
      "Epoch 8   | Training Accuracy: 0.9829999804496765\n",
      "Epoch 9   | Training Accuracy: 0.9819999933242798\n",
      "Epoch 10   | Training Accuracy: 0.9829999804496765\n",
      "Epoch 11   | Training Accuracy: 0.9879999756813049\n",
      "Epoch 12   | Training Accuracy: 0.9869999885559082\n",
      "Epoch 13   | Training Accuracy: 0.9879999756813049\n",
      "Epoch 14   | Training Accuracy: 0.9900000095367432\n",
      "Epoch 15   | Training Accuracy: 0.9890000224113464\n",
      "Epoch 16   | Training Accuracy: 0.9909999966621399\n",
      "Epoch 17   | Training Accuracy: 0.9900000095367432\n",
      "Epoch 18   | Training Accuracy: 0.9900000095367432\n",
      "Epoch 19   | Training Accuracy: 0.9909999966621399\n",
      "Epoch 20   | Training Accuracy: 0.9869999885559082\n",
      "Epoch 21   | Training Accuracy: 0.9900000095367432\n",
      "Epoch 22   | Training Accuracy: 0.9909999966621399\n",
      "Epoch 23   | Training Accuracy: 0.9909999966621399\n",
      "Epoch 24   | Training Accuracy: 0.9909999966621399\n",
      "Epoch 25   | Training Accuracy: 0.9919999837875366\n",
      "Epoch 26   | Training Accuracy: 0.9919999837875366\n",
      "Epoch 27   | Training Accuracy: 0.9919999837875366\n",
      "Epoch 28   | Training Accuracy: 0.9919999837875366\n",
      "Epoch 29   | Training Accuracy: 0.9919999837875366\n",
      "Epoch 30   | Training Accuracy: 0.9909999966621399\n",
      "Epoch 31   | Training Accuracy: 0.9909999966621399\n",
      "Epoch 32   | Training Accuracy: 0.9909999966621399\n",
      "Epoch 33   | Training Accuracy: 0.9900000095367432\n",
      "Epoch 34   | Training Accuracy: 0.9919999837875366\n",
      "Epoch 35   | Training Accuracy: 0.9900000095367432\n",
      "Epoch 36   | Training Accuracy: 0.9919999837875366\n",
      "Epoch 37   | Training Accuracy: 0.9929999709129333\n",
      "Epoch 38   | Training Accuracy: 0.9919999837875366\n",
      "Epoch 39   | Training Accuracy: 0.9909999966621399\n",
      "Epoch 40   | Training Accuracy: 0.9909999966621399\n",
      "Epoch 41   | Training Accuracy: 0.9919999837875366\n",
      "Epoch 42   | Training Accuracy: 0.9919999837875366\n",
      "Epoch 43   | Training Accuracy: 0.9909999966621399\n",
      "Epoch 44   | Training Accuracy: 0.9919999837875366\n",
      "Epoch 45   | Training Accuracy: 0.9909999966621399\n",
      "Epoch 46   | Training Accuracy: 0.9929999709129333\n",
      "Epoch 47   | Training Accuracy: 0.9919999837875366\n",
      "Epoch 48   | Training Accuracy: 0.9940000176429749\n",
      "Epoch 49   | Training Accuracy: 0.9929999709129333\n",
      "Training Done\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    #------Training DAtaset------------#\n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        batch_x, batch_y=next_batch(batch_size=size_of_batch, data=x_train, labels=y_train)\n",
    "        \n",
    "        feed_dict= {X: batch_x, Y: batch_y}\n",
    "        sess.run(train_step, feed_dict=feed_dict)\n",
    "        \n",
    "    s, batch_accuracy=sess.run(fetches=[merged_summary, accuracy], feed_dict=feed_dict)\n",
    "    \n",
    "    train_writer.add_summary(s, epoch)\n",
    "    \n",
    "    print(f'Epoch {epoch}   | Training Accuracy: {batch_accuracy}')\n",
    "    \n",
    "    #------Validation Dataset-----------#\n",
    "    \n",
    "    summary=sess.run(fetches=merged_summary, feed_dict={X:x_val, Y:y_val})\n",
    "    validation_writer.add_summary(summary, epoch) \n",
    "print('Training Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "img=Image.open('MNIST/test_img.png')\n",
    "bw= img.convert('L')\n",
    "img_array=np.invert(bw)\n",
    "img_array.shape\n",
    "test_img=img_array.ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = sess.run(fetches=tf.argmax(output, axis=1), feed_dict={X: [test_img]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_test[1])==prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set is  97.78%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy=sess.run(fetches=accuracy, feed_dict={X: x_test, Y: y_test})\n",
    "print(f'Accuracy on test set is {test_accuracy: 0.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resetting for the next run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_writer.close()\n",
    "validation_writer.close()\n",
    "sess.close()\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code for first part of the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.name_scope('hidden_layer_1'):\n",
    "#     initial_w1=tf.truncated_normal(shape=(TOTAL_INPUTS, n_hidden1), seed=42, stddev=0.1)\n",
    "#     w1=tf.Variable(initial_w1, name='w1')\n",
    "\n",
    "#     initial_b1=tf.constant(shape=[n_hidden1], value=0.0)\n",
    "#     b1=tf.Variable(initial_b1, name='b1')\n",
    "\n",
    "#     layer1_in = tf.matmul(X, w1) + b1\n",
    "#     layer1_out= tf.nn.relu(layer1_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.name_scope('hidden_layer_2'):\n",
    "#     initial_b2=tf.constant(shape=[n_hidden2], value=0.0)\n",
    "#     b2=tf.Variable(initial_b2, name='b2')\n",
    "\n",
    "#     initial_w2=tf.truncated_normal(shape=(n_hidden1, n_hidden2), seed=42, stddev=0.1)\n",
    "#     w2=tf.Variable(initial_w2, name='w2')\n",
    "\n",
    "#     layer2_in=tf.matmul(layer1_out, w2) + b2\n",
    "#     layer2_out=tf.nn.relu(layer2_in)\n",
    "\n",
    "# with tf.name_scope('output_layer'):\n",
    "#     initial_w_out=tf.truncated_normal(shape=(n_hidden2, NUM_CLASSES), seed=42, stddev=0.1)\n",
    "#     w_out=tf.Variable(initial_w_out, name='w_out')\n",
    "\n",
    "#     initial_b_out=tf.constant(shape=[NUM_CLASSES], value=0.0)\n",
    "#     b_out=tf.Variable(initial_b_out, name='b_out')\n",
    "\n",
    "#     layer3_in=tf.matmul(layer2_out, w_out) +b_out\n",
    "#     output=tf.nn.softmax(layer3_in)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
